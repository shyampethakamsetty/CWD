from crewai import Agent
from typing import Dict, Optional, List, Any
import logging
from datetime import datetime
import os
import sys
import yfinance as yf
import json
from tqdm import tqdm
import time

# Configure logging with more detailed formatting
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Setup logging directory and file
log_dir = os.path.join('Yahoo', 'Logs')
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f'{datetime.now().strftime("%Y-%m-%d")}_context_agent.log')
file_handler = logging.FileHandler(agent_log_file)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

class ConfigurationManager:
    """Configuration management for the ContextAgent"""
    
    @staticmethod
    def setup_directories() -> Dict[str, str]:
        """Create and validate necessary directories."""
        date_now = datetime.now().strftime('%Y-%m-%d')
        log_dir = os.path.join('Yahoo', 'Logs')
        output_dir = os.path.join('Yahoo', 'Outputs')
        
        try:
            os.makedirs(log_dir, exist_ok=True)
            logger.info(f"Log directory validated: {log_dir}")
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"Output directory validated: {output_dir}")
            
            return {
                'LOG_FILE': os.path.join(log_dir, f'{date_now}_stock_price_fetcher.log'),
                'OUTPUT_DIR': output_dir,
                'DATE': date_now
            }
        except Exception as e:
            logger.error(f"Error setting up directories: {str(e)}")
            raise
    
    @staticmethod
    def validate_config(config: Dict[str, Any]) -> bool:
        """Validate configuration parameters."""
        required_fields = {
            'RETRY_ATTEMPTS': (int, lambda x: x > 0),
            'RETRY_DELAY': ((int, float), lambda x: x >= 0),
            'RATE_LIMIT_DELAY': ((int, float), lambda x: x >= 0),
            'BATCH_SIZE': (int, lambda x: x > 0)
        }
        
        try:
            for field, (expected_type, validator) in required_fields.items():
                if field not in config:
                    logger.error(f"Missing required configuration field: {field}")
                    return False
                    
                if not isinstance(config[field], expected_type):
                    logger.error(f"Invalid type for {field}. Expected {expected_type}, got {type(config[field])}")
                    return False
                    
                if not validator(config[field]):
                    logger.error(f"Invalid value for {field}: {config[field]}")
                    return False
            
            logger.info("Configuration validation successful")
            return True
            
        except Exception as e:
            logger.error(f"Error during configuration validation: {str(e)}")
            return False
    
    @staticmethod
    def validate_symbols(symbols: List[str]) -> bool:
        """Validate stock symbols list."""
        try:
            if not symbols:
                logger.error("Symbols list cannot be empty")
                return False
                
            if not all(isinstance(symbol, str) for symbol in symbols):
                logger.error("All symbols must be strings")
                return False
                
            if not all(symbol.strip() for symbol in symbols):
                logger.error("Symbols cannot be empty strings")
                return False
                
            if len(symbols) != len(set(symbols)):
                logger.error("Duplicate symbols found in the list")
                return False
            
            logger.info(f"Symbol list validation successful. Total symbols: {len(symbols)}")
            return True
            
        except Exception as e:
            logger.error(f"Error during symbols validation: {str(e)}")
            return False

class StockDataFetcher:
    """Handles stock data fetching operations"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        logger.info("StockDataFetcher initialized with configuration")
    
    def validate_price(self, price: float) -> bool:
        """Validate if the price is reasonable."""
        return isinstance(price, (int, float)) and price > 0

    def fetch_last_close(self, symbol: str) -> Optional[float]:
        """Fetch last close price for a given symbol with retries."""
        for attempt in range(self.config['RETRY_ATTEMPTS']):
            try:
                stock = yf.Ticker(symbol)
                history = stock.history(period="1d")
                
                if history.empty:
                    logger.warning(f"No data available for {symbol}")
                    return None
                    
                last_close = history['Close'].iloc[-1]
                
                if not self.validate_price(last_close):
                    logger.warning(f"Invalid price for {symbol}: {last_close}")
                    return None
                    
                logger.debug(f"Successfully fetched {symbol} Last Close: {last_close}")
                return last_close
                
            except Exception as e:
                logger.error(f"Error fetching {symbol} (Attempt {attempt+1}/{self.config['RETRY_ATTEMPTS']}): {str(e)}")
                if attempt < self.config['RETRY_ATTEMPTS'] - 1:
                    time.sleep(self.config['RETRY_DELAY'])
        
        return None

    def process_batch(self, symbols_batch: List[str]) -> Dict[str, float]:
        """Process a batch of symbols with rate limiting."""
        batch_data = {}
        for symbol in tqdm(symbols_batch, desc="Processing batch", unit="stock"):
            last_close = self.fetch_last_close(symbol)
            if last_close is not None:
                batch_data[symbol] = round(last_close, 2)
            time.sleep(self.config['RATE_LIMIT_DELAY'])
        return batch_data

    def save_data(self, data: Dict[str, float], date: str) -> bool:
        """Save data to JSON file with error handling."""
        try:
            filename = os.path.join(self.config['OUTPUT_DIR'], f'{date}_stock_last_close.json')
            with open(filename, 'w') as f:
                json.dump(data, f, indent=4)
            logger.info(f"Successfully saved data to {filename}")
            return True
        except Exception as e:
            logger.error(f"Error saving data: {str(e)}")
            return False

from typing import ClassVar, List

class ContextAgent(Agent):
    """Agent for fetching and maintaining stock price context"""
    
    # Class constants
    SYMBOLS: ClassVar[List[str]] = [
        "AAPL", "MSFT", "AMZN", "GOOGL", "META", "TSLA", "NVDA", "PYPL", "INTC", "CMCSA",
        "NFLX", "ADBE", "PEP", "CSCO", "AVGO", "TXN", "COST", "TMUS", "AMGN", "SBUX",
        "QCOM", "GILD", "MDLZ", "INTU", "ISRG", "VRTX", "REGN", "ILMN", "ADI",
        "CSX", "MU", "BKNG", "AMAT", "ADP", "MNST", "MELI", "ADSK", "JD", "LRCX",
        "EBAY", "KHC", "BIIB", "LULU", "EA", "WDAY", "PCAR", "DXCM", "CTSH", "MRVL",
        "CRM"
    ]
      # Default configuration
    DEFAULT_CONFIG: ClassVar[Dict[str, Any]] = {
        'RETRY_ATTEMPTS': 3,
        'RETRY_DELAY': 5,
        'RATE_LIMIT_DELAY': 1,
        'BATCH_SIZE': 50
    }
      def __init__(self):
        """Initialize the ContextAgent."""
        logger.info("Initializing ContextAgent")
        try:
            # Initialize base Agent first
            super().__init__(
                role='Stock Price Context Generator',
                goal='Generate accurate last closing price data for stocks to provide market context',
                backstory="""I am a specialized agent that fetches and maintains up-to-date stock price data.
                I collect closing prices for major tech stocks to provide critical market context for analysis.""",
                verbose=True
            )
            
            # Setup working directories
            self.output_dir = os.path.join('Yahoo', 'Outputs')
            os.makedirs(self.output_dir, exist_ok=True)
            
            # Initialize configuration
            self._setup_config()
            
            logger.info("ContextAgent initialized successfully")
            logger.info(f"Number of symbols to process: {len(self.SYMBOLS)}")
            logger.info(f"Output directory: {self.output_dir}")
            
            logger.info("ContextAgent initialized successfully")
            logger.info(f"Agent configured with batch size: {self.config['BATCH_SIZE']}")
            logger.info(f"Number of symbols to process: {len(self.SYMBOLS)}")
            logger.info(f"Output directory: {self.config['OUTPUT_DIR']}")
            
        except Exception as e:
            logger.error(f"Error initializing ContextAgent: {str(e)}", exc_info=True)
            raise

    def execute_task(self, context: list = None) -> Dict[str, float]:
        """
        Execute the stock data collection task.
        
        Args:
            context (list, optional): List of context strings for the task. Defaults to None.
        
        Returns:
            Dict[str, float]: Dictionary of stock symbols and their last closing prices
        """
        execution_start = datetime.now()
        logger.info(f"Starting stock data collection process at {execution_start}")
        logger.info(f"Task context: {context}")
        
        try:
            stock_data = {}
            failed_symbols = []
            total_batches = (len(self.SYMBOLS) + self.config['BATCH_SIZE'] - 1) // self.config['BATCH_SIZE']
            
            # Process symbols in batches with progress tracking
            for batch_num, i in enumerate(range(0, len(self.SYMBOLS), self.config['BATCH_SIZE']), 1):
                batch = self.SYMBOLS[i:i + self.config['BATCH_SIZE']]
                batch_start_time = datetime.now()
                
                logger.info(f"Processing batch {batch_num}/{total_batches}")
                logger.info(f"Batch {batch_num} details:")
                logger.info(f"  - Symbols: {', '.join(batch)}")
                logger.info(f"  - Batch size: {len(batch)}")
                
                try:
                    batch_data = self.fetcher.process_batch(batch)
                    stock_data.update(batch_data)
                    
                    # Calculate batch statistics
                    batch_end_time = datetime.now()
                    batch_duration = batch_end_time - batch_start_time
                    success_count = len(batch_data)
                    success_rate = (success_count / len(batch)) * 100
                    
                    # Track failed symbols
                    batch_failed = [symbol for symbol in batch if symbol not in batch_data]
                    if batch_failed:
                        failed_symbols.extend(batch_failed)
                        logger.warning(f"Failed symbols in batch {batch_num}: {', '.join(batch_failed)}")
                    
                    # Log batch results
                    logger.info(f"Batch {batch_num} completed:")
                    logger.info(f"  - Success rate: {success_rate:.2f}% ({success_count}/{len(batch)})")
                    logger.info(f"  - Duration: {batch_duration}")
                    
                    # Overall progress
                    total_processed = len(stock_data)
                    overall_progress = (total_processed / len(self.SYMBOLS)) * 100
                    logger.info(f"Overall progress: {overall_progress:.2f}% ({total_processed}/{len(self.SYMBOLS)})")
                    
                except Exception as e:
                    logger.error(f"Critical error in batch {batch_num}: {str(e)}")
                    failed_symbols.extend(batch)
                    continue
            
            # Save and report results
            success_rate = len(stock_data) / len(self.SYMBOLS) * 100 if self.SYMBOLS else 0
            
            if stock_data:
                if self.fetcher.save_data(stock_data, self.config['DATE']):
                    execution_end = datetime.now()
                    execution_time = execution_end - execution_start
                    
                    # Generate execution summary
                    summary = f"""Task Execution Summary:
                    - Total symbols processed: {len(self.SYMBOLS)}
                    - Successfully fetched: {len(stock_data)} ({success_rate:.2f}%)
                    - Failed symbols: {len(failed_symbols)}
                    - Execution time: {execution_time}
                    - Output file: {os.path.join(self.config['OUTPUT_DIR'], f"{self.config['DATE']}_stock_last_close.json")}"""
                    
                    logger.info(summary)
                    
                    if failed_symbols:
                        logger.warning(f"Failed symbols: {', '.join(failed_symbols)}")
            else:
                logger.error("No stock data was fetched")
            
            return stock_data
            
        except Exception as e:
            logger.error(f"Critical error during task execution: {str(e)}", exc_info=True)
            raise
